# sync data to cloud storage
5 * * * * /bin/bash -l -c 'cd /home/igrigorik/githubarchive.org/crawler/data && gzip -9 `date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json && gsutil -h "Content-Type: application/x-gzip" cp -a public-read `date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json.gz gs://data.githubarchive.org/' >> /home/igrigorik/githubarchive.org/crawler/data/sync.log 2>&1

# import data into bigquery
8 * * * * /bin/bash -l -c 'cd /home/igrigorik/githubarchive.org/bigquery     && ruby upload.rb -i ../crawler/data/`date +"\%Y-\%m-\%d-\%-k" -d "1 hour ago"`.json.gz' >> /home/igrigorik/githubarchive.org/crawler/data/sync.log 2>&1

# append daily results to monthly rollup
10 0 * * * /bin/bash -l -c 'bq query --append_table --allow_large_results --destination_table=month.`date +"\%Y\%m" -d"1 day ago"` "SELECT * FROM [githubarchive:day.events_`date +"\%Y\%m\%d" -d"1 day ago"`]"' >> /home/igrigorik/githubarchive.org/crawler/data/append.log 2>&1

# keep last 365 days worth of data
0 0 * * * find /home/archiver/githubarchive/crawler/data/* -mtime +365 -exec rm {} \;

